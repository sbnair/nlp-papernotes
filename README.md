# nlp-papernotes

> Reading NLP-related papers when working, summary and thoughts. Major in Machine Reading Comprehension (MRC), Question Answering (QA), Multi-document MRC, Information Retriever (IR) and Ranking.

## Language Model

Summary [PPT](#).

- [BERT](#)
- [XLNet](#)
- [structBERT](#)
- [spanBERT](#)
- [SemBERT](#)
- [RoBERTa](#)
- [ALBERT](#)

## MRC & QA

Summary [PPT](#).

### ACL 2019
- [RE3QA](#)
- [RankQA](#)

## Knowledge Distillation (KD)
KD is similar to [Label Smoothing](https://medium.com/@nainaakash012/when-does-label-smoothing-help-89654ec75326), introduction of KD from the following blogs:

- [Neural Network Distiller](https://nervanasystems.github.io/distiller/knowledge_distillation.html)
- [Smaller, faster, cheaper, lighter: Introducing DistilBERT, a distilled version of BERT](https://medium.com/huggingface/distilbert-8cf3380435b5)
- [Tap into the dark knowledge using neural nets — Knowledge distillation](https://towardsdatascience.com/knowledge-distillation-and-the-concept-of-dark-knowledge-8b7aed8014ac)
- [Knowledge Distillation](https://medium.com/neuralmachine/knowledge-distillation-dc241d7c2322)

## MS MARCO
MS MARCO [LeaderBoard](http://www.msmarco.org/leaders.aspx), several public paper models, written on Github [MSMARCO-MRC-Analysis](https://github.com/IndexFziQ/MSMARCO-MRC-Analysis)

### Passage Retrieval
- Duet: [[arxiv](https://arxiv.org/abs/1610.08136)], [[PPT](#)], [[notes](#)] 
- [Duet V2](#)

## Bookshelf
- 《[Deep Learning](http://www.deeplearningbook.org/)》
- 《[Deep Learning (中文版)](https://exacity.github.io/deeplearningbook-chinese/)》
- 《[Mathematics for Machine Learning](https://mml-book.github.io/)》
- 《[Learning to rank for information retrieval](https://www.cda.cn/uploadfile/image/20151220/20151220115436_46293.pdf)》
- 《[GDL-Generative Deep Learning](#)》
- 《[Introduction to Information Retrieval](https://nlp.stanford.edu/IR-book/)》